import os
import re
import time
import tempfile
import subprocess
from typing import Optional, Tuple
import json
import numpy as np
import pandas as pd
import PyPDF2
import pyaudio
import soundfile as sf
import pygame
import torch
from langdetect import detect_langs
from transformers import (
    AutoModelForSpeechSeq2Seq,
    AutoProcessor,
    pipeline
)
from kokoro import KPipeline
from ollama import Client
import streamlit as st
from streamlit_chat import message as st_chat_message
from PIL import Image
from googletrans import Translator
from deep_translator import GoogleTranslator
from duckduckgo_search import DDGs

st.set_page_config(
    page_title="Chatbot",
    page_icon="ğŸ’¬",
    layout="centered"
)
st.title("Chat with your own personal Chatbot!")

init_container = st.empty()

# Model settings
DEFAULT_MODEL = "gemma3:12b"
WHISPER_MODEL_ID = "openai/whisper-large-v3-turbo"

TEXT_MAX_CHARS = 30000  # Maximum characters for text content
DATAFRAME_MAX_CHARS = 30000  # Maximum characters for dataframe string representation

# System prompts for different languages
ENGLISH_SYSTEM_PROMPT = """
Always respond in English regardless of input language.
Keep answers easy to read and understand with as few words as possible.
Pay attention to the conversation context and reference previous turns when relevant.
"""

CHINESE_SYSTEM_PROMPT = """
ç„¡è«–è¼¸å…¥èªè¨€å¦‚ä½•ï¼Œå§‹çµ‚ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
ä¿æŒå›ç­”ç°¡æ½”æ˜äº†ã€‚
æ³¨æ„å°è©±ä¸Šä¸‹æ–‡ï¼Œåœ¨ç›¸é—œæ™‚å¼•ç”¨ä¹‹å‰çš„å°è©±å…§å®¹ã€‚
"""

# Data analysis prompts
DATAFRAME_ANALYSIS_PROMPT_EN = """
You are a helpful assistant specialized in analyzing pandas DataFrames.
Analyze the pandas DataFrame below and answer the user's question.
Keep your answers concise and easy to read and understand, like explaining to a child.
Give simple examples if possible.

DataFrame:
```
{dataframe_string}
```

User Question: {user_prompt}

Answer:
"""

DATAFRAME_ANALYSIS_PROMPT_ZH = """
ä½ æ˜¯ä¸€å€‹å°ˆé–€åˆ†æpandas DataFrameçš„åŠ©æ‰‹ã€‚
è«‹åˆ†æä»¥ä¸‹DataFrameä¸¦ç”¨ç¹é«”ä¸­æ–‡å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚
ä¿æŒå›ç­”ç°¡æ½”æ˜“è®€æ˜“æ‡‚ï¼Œå°±åƒå‘å°å­©è§£é‡‹ä¸€æ¨£ã€‚
å¦‚æœå¯èƒ½çš„è©±ï¼Œçµ¦å‡ºç°¡å–®çš„ä¾‹å­ã€‚

DataFrame:
```
{dataframe_string}
```

ç”¨æˆ¶å•é¡Œ: {user_prompt}

å›ç­”:
"""

TEXT_ANALYSIS_PROMPT_EN = """
You are a helpful assistant specialized in analyzing text content.
Analyze the text below and answer the user's question.
Keep your answers concise and easy to read and understand, like explaining to a child.
Give simple examples if possible.

Text Content:
```
{text_content_string}
```

User Question: {user_prompt}

Answer:
"""

TEXT_ANALYSIS_PROMPT_ZH = """
ä½ æ˜¯ä¸€å€‹å°ˆé–€åˆ†ææ–‡æœ¬å…§å®¹çš„åŠ©æ‰‹ã€‚
è«‹ç”¨ç¹é«”ä¸­æ–‡åˆ†æä»¥ä¸‹æ–‡æœ¬ä¸¦å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚
ä¿æŒå›ç­”ç°¡æ½”æ˜“è®€æ˜“æ‡‚ï¼Œå°±åƒå‘å°å­©è§£é‡‹ä¸€æ¨£ã€‚
å¦‚æœå¯èƒ½çš„è©±ï¼Œçµ¦å‡ºç°¡å–®çš„ä¾‹å­ã€‚

æ–‡æœ¬å…§å®¹:
```
{text_content_string}
```

ç”¨æˆ¶å•é¡Œ: {user_prompt}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”:"""

# Chain of thought prompts
COT_SYSTEM_PROMPT = """You are an expert AI assistant that explains your reasoning step by step. For each step:
1. Provide a clear title describing what you're examining
2. Give detailed analysis and reasoning
3. Consider multiple possibilities and approaches
4. Evaluate evidence and assumptions
5. Draw logical conclusions

USE AT LEAST 3 DIFFERENT APPROACHES:
- Direct analysis
- Alternative perspectives
- Edge cases and limitations
- Comparative analysis
- Validation and verification

CONSIDER LIMITATIONS:
- Assumptions and biases
- Data completeness
- Alternative interpretations
- Potential errors

Format your response in JSON with:
- 'title': Step title
- 'content': Detailed reasoning
- 'next_action': Either 'continue' or 'final_answer'
"""

COT_DATA_SYSTEM_PROMPT = """You are an expert AI assistant analyzing data. For each step:
1. Provide a clear title describing what you're examining
2. Give detailed analysis of the data
3. Consider multiple interpretations
4. Evaluate patterns and relationships
5. Draw data-driven conclusions

USE AT LEAST 3 DIFFERENT ANALYTICAL APPROACHES:
- Statistical analysis
- Pattern recognition
- Trend identification
- Outlier detection
- Data validation

CONSIDER LIMITATIONS:
- Data quality and completeness
- Statistical significance
- Potential biases
- Alternative interpretations

Format your response in JSON with:
- 'title': Step title
- 'content': Detailed analysis
- 'next_action': Either 'continue' or 'final_answer'
"""

# Audio configuration
SAMPLE_RATE = 16000
AUDIO_CHANNELS = 1
CHUNK_SIZE = 1024
AUDIO_FORMAT = pyaudio.paInt16
CHUNK_DURATION = CHUNK_SIZE / SAMPLE_RATE

# Recording configuration
SILENCE_THRESHOLD = 500
SILENCE_DURATION = 3  # seconds before stopping
MAX_RECORDING_DURATION = 30  # maximum recording duration in seconds

# Upload options
UPLOAD_OPTIONS = ["None", "Text File (TXT/PDF)", "Spreadsheet (CSV/Excel)", "Image (JPG/PNG)"]

# Chain of thought configuration
COT_CONFIG = {
    "temperature": 0.2,
    "max_steps": 5,
    "step_max_tokens": 300,
    "final_answer_max_tokens": 200,
    "max_retries": 3,
    "retry_delay": 0.5
}

# Default LLM configuration
DEFAULT_LLM = {
    "temperature": 0.7,
    "top_k": 40,
    "top_p": 0.9
}

# Search configuration
SEARCH_CONFIG = {
    "max_results": 3,  # Number of search results to fetch per query
    "max_iterations": 3,  # Number of search iterations
    "min_confidence_threshold": 0.7,  # Threshold for when to dig deeper
    "max_query_length": 150,  # Maximum length of generated search queries
    "region": "wt-wt",  # Worldwide results
    "safesearch": "moderate"
}

# PDF analysis prompts
PDF_ANALYSIS_PROMPT_EN = """
You are a helpful assistant specialized in analyzing PDF documents.
Analyze the PDF content below and answer the user's question.
Keep your answers concise and easy to read and understand, like explaining to a child.
Give simple examples if possible.

PDF Content:
```
{pdf_content_string}
```

User Question: {user_prompt}

Answer:
"""

PDF_ANALYSIS_PROMPT_ZH = """
ä½ æ˜¯ä¸€å€‹å°ˆé–€åˆ†æPDFæ–‡ä»¶çš„åŠ©æ‰‹ã€‚
è«‹åˆ†æä»¥ä¸‹PDFå…§å®¹ä¸¦ç”¨ç¹é«”ä¸­æ–‡å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚
ä¿æŒå›ç­”ç°¡æ½”æ˜“è®€æ˜“æ‡‚ï¼Œå°±åƒå‘å°å­©è§£é‡‹ä¸€æ¨£ã€‚
å¦‚æœå¯èƒ½çš„è©±ï¼Œçµ¦å‡ºç°¡å–®çš„ä¾‹å­ã€‚

PDFå…§å®¹:
```
{pdf_content_string}
```

ç”¨æˆ¶å•é¡Œ: {user_prompt}

å›ç­”:
"""
# --- 9. Helper Functions ---
def generate_search_query(topic, previous_findings=None, knowledge_gaps=None):
    """Generate a focused search query based on topic and previous research."""
    try:
        # Create a more direct prompt that emphasizes the original question
        language = st.session_state.fixed_language
        
        if language == 'zh':
            prompt = f"""åŸå§‹å•é¡Œ: {topic}
æ‚¨çš„ä»»å‹™: ç”Ÿæˆä¸€å€‹ç°¡å–®ç›´æ¥çš„æœç´¢æŸ¥è©¢ï¼ˆ5-10å€‹è©ï¼‰ä»¥æŸ¥æ‰¾æœ‰é—œæ­¤ä¸»é¡Œçš„ä¿¡æ¯ã€‚
- åªé—œæ³¨åŸå§‹å•é¡Œä¸­çš„ä¸»è¦æ¦‚å¿µ
- ä¸è¦åŒ…å«åƒã€Œé€™æ˜¯ã€æˆ–ã€Œæœç´¢æŸ¥è©¢ã€ç­‰çŸ­èª
- ä¸è¦ä½¿ç”¨å¼•è™Ÿæˆ–ç‰¹æ®Šå­—ç¬¦
- åªå¯«å¯¦éš›çš„æœç´¢è©ï¼Œæ²’æœ‰å…¶ä»–å…§å®¹
- è¦å…·é«”ä¸¦åœ¨é©ç•¶æ™‚ä½¿ç”¨é†«å­¸/æŠ€è¡“è¡“èª

å¥½çš„æŸ¥è©¢ç¤ºä¾‹:
- DOAC VKA å¿ƒåŒ…å¡«å¡ æ¯”è¼ƒ
- é˜¿å¸åŒ¹æ— æŠ—ç‚ ä½œç”¨æ©Ÿåˆ¶
- æ°£å€™è®ŠåŒ– çŠç‘šç¤ å½±éŸ¿

ä¸å¥½çš„æŸ¥è©¢ç¤ºä¾‹:
- ã€Œé€™æ˜¯é—œæ–¼DOACçš„æœç´¢æŸ¥è©¢ã€
- é€™è£¡æœ‰å…©å€‹å¯èƒ½çš„æœç´¢æŸ¥è©¢
- æˆ‘å°‡æœç´¢æœ‰é—œçš„ä¿¡æ¯

æ‚¨çš„æœç´¢æŸ¥è©¢:"""
        else:
            prompt = f"""ORIGINAL QUESTION: {topic}
Your task: Generate a simple, direct search query (5-10 words) to find information about this topic.
- Focus ONLY on the main concepts from the original question
- Do NOT include phrases like "here is" or "search query"
- Do NOT use quotes or special characters
- Just write the actual search terms, nothing else
- Be specific and use medical/technical terms when appropriate

Example good queries:
- DOAC VKA cardiac tamponade comparison
- aspirin mechanism of action inflammation
- climate change impact coral reefs

Example bad queries:
- "Here is a search query about DOAC"
- Here are two possible search queries
- I will search for information about

YOUR SEARCH QUERY:"""
        
        response = st.session_state.ollama_client.chat(
            model=st.session_state.selected_model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æœç´¢æŸ¥è©¢ç”Ÿæˆå™¨ã€‚åªç”Ÿæˆæœç´¢è©ï¼Œæ²’æœ‰å…¶ä»–å…§å®¹ã€‚" if language == 'zh' else "You are a search query generator. Generate only the search terms, nothing else."},
                {"role": "user", "content": prompt}
            ]
        )
        
        # Clean up the generated query
        query = response['message']['content'].strip()
        query = query.replace('"', '').replace("'", "").replace("(", "").replace(")", "")
        
        # Remove common prefixes that the model might still include
        prefixes_to_remove = [
            "Here is", "Here are", "Search query:", "Query:", "YOUR SEARCH QUERY:", 
            "A good search query would be", "I would search for", "Search for",
            "ä»¥ä¸‹æ˜¯", "æŸ¥è©¢:", "æœç´¢æŸ¥è©¢:", "ä¸€å€‹å¥½çš„æœç´¢æŸ¥è©¢æ˜¯", "æˆ‘æœƒæœç´¢", "æœç´¢"
        ]
        
        for prefix in prefixes_to_remove:
            if query.startswith(prefix):
                query = query[len(prefix):].strip()
        
        # If query is too long, take just the first sentence or limit to 15 words
        if len(query.split()) > 15:
            if "." in query:
                query = query.split(".")[0].strip()
            else:
                query = " ".join(query.split()[:15])
        
        # Print the original question and the search query for clarity
        if language == 'zh':
            st.markdown(f"*åŸå§‹å•é¡Œ:* {topic}")
            st.markdown(f"*ç”Ÿæˆçš„æœç´¢è©:* {query}")
        else:
            st.markdown(f"*Original Question:* {topic}")
            st.markdown(f"*Generated Search Terms:* {query}")
        
        return query
    except Exception as e:
        st.warning(f"Error generating search query: {e}")
        # Fallback to a simplified version of the original question
        simple_query = " ".join(topic.split()[:10])
        return simple_query

def analyze_findings(topic, current_findings, iteration):
    """Analyze current findings and identify knowledge gaps."""
    try:
        language = st.session_state.fixed_language
        
        if language == 'zh':
            prompt = f"""åŸºæ–¼æˆ‘å€‘å° '{topic}' çš„ç ”ç©¶ï¼Œåˆ†æä»¥ä¸‹ç™¼ç¾ï¼š

{current_findings}

å°æ–¼è¿­ä»£ {iteration}ï¼Œè«‹ï¼š
1. ç¸½çµæˆ‘å€‘å­¸åˆ°çš„è¦é»
2. ç¢ºå®šç‰¹å®šçš„çŸ¥è­˜å·®è·æˆ–éœ€è¦æ›´å¤šç´°ç¯€çš„é ˜åŸŸ
3. å»ºè­°æˆ‘å€‘æ¥ä¸‹ä¾†æ‡‰è©²èª¿æŸ¥å“ªäº›å…·é«”æ–¹é¢

è«‹å°‡æ‚¨çš„å›æ‡‰æ ¼å¼åŒ–ç‚ºJSONï¼Œä½¿ç”¨éµï¼š'summary'ï¼ˆæ‘˜è¦ï¼‰ã€'gaps'ï¼ˆå·®è·ï¼‰ã€'next_focus'ï¼ˆä¸‹ä¸€å€‹ç„¦é»ï¼‰"""
        else:
            prompt = f"""Based on our research about '{topic}', analyze these findings:

{current_findings}

For iteration {iteration}, please:
1. Summarize the key points we've learned
2. Identify specific knowledge gaps or areas that need more detail
3. Suggest what specific aspects we should investigate next

Format your response as JSON with keys: 'summary', 'gaps', 'next_focus'"""

        response = st.session_state.ollama_client.chat(
            model=st.session_state.selected_model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹ç ”ç©¶åˆ†æå¸«ï¼Œè­˜åˆ¥çŸ¥è­˜å·®è·ã€‚" if language == 'zh' else "You are a research analyst identifying knowledge gaps."},
                {"role": "user", "content": prompt}
            ],
            format='json'
        )
        
        analysis_result = json.loads(response['message']['content'])
        
        # Ensure summary is a string
        summary = analysis_result.get('summary', '')
        if not isinstance(summary, str):
            summary = str(summary)  # Convert to string if not
        
        return {
            "summary": summary,
            "gaps": analysis_result.get('gaps', "æ²’æœ‰è­˜åˆ¥å‡ºå·®è·" if language == 'zh' else "No gaps identified"),
            "next_focus": analysis_result.get('next_focus', topic)
        }
    except Exception as e:
        st.warning(f"Error analyzing findings: {e}")
        return {
            "summary": "åˆ†æå¤±æ•—" if st.session_state.fixed_language == 'zh' else "Analysis failed",
            "gaps": "åˆ†æå¤±æ•—" if st.session_state.fixed_language == 'zh' else "Analysis failed",
            "next_focus": topic
        }
    
def deep_research(query, language='en'):
    """Perform iterative deep research on a topic."""
    all_findings = []
    current_summary = ""
    all_sources = []  # Track all sources used
    max_attempts = 6  # Maximum number of attempts to get meaningful results
    
    # Create a container to capture the research process
    research_container = st.empty()
    with research_container, st.expander("ğŸ” Research Process", expanded=True):
        iteration = 0
        while iteration < SEARCH_CONFIG['max_iterations']:  # Changed from RESEARCH_CONFIG
            attempt = 0
            meaningful_iteration = False
            
            while not meaningful_iteration and attempt < max_attempts:
                attempt += 1
                st.markdown(f"**Iteration {iteration + 1} (Attempt {attempt})**")
                
                # Generate search query
                search_query = generate_search_query(
                    query, 
                    current_summary if current_summary else None,
                    all_findings[-1].get('gaps', None) if all_findings else None
                )
                
                st.markdown(f"*Search Query:* {search_query}")
                
                # Perform search and display sources
                search_results = get_web_search_results(
                    search_query,
                    language=language
                )
                
                if not search_results:
                    st.warning("No information found. Trying a different query...")
                    continue
                
                # Extract sources
                sources = []
                for line in search_results.split('\n'):
                    if 'Source:' in line or 'ä¾†æº:' in line:
                        url = line.split(': ')[-1].strip()
                        if url not in all_sources:  # Only add unique sources
                            sources.append(url)